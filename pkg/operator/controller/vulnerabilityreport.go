package controller

import (
	"context"
	"fmt"
	"reflect"

	"github.com/aquasecurity/starboard/pkg/apis/aquasecurity/v1alpha1"
	"github.com/aquasecurity/starboard/pkg/docker"
	"github.com/aquasecurity/starboard/pkg/kube"
	"github.com/aquasecurity/starboard/pkg/operator/etc"
	. "github.com/aquasecurity/starboard/pkg/operator/predicate"
	"github.com/aquasecurity/starboard/pkg/vulnerabilityreport"
	"github.com/go-logr/logr"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/utils/pointer"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/builder"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
)

type VulnerabilityReportReconciler struct {
	logr.Logger
	etc.Config
	client.Client
	kube.ObjectResolver
	LimitChecker
	kube.LogsReader
	kube.SecretsReader
	vulnerabilityreport.Plugin
	vulnerabilityreport.ReadWriter
}

func (r *VulnerabilityReportReconciler) SetupWithManager(mgr ctrl.Manager) error {
	installModePredicate, err := InstallModePredicate(r.Config)
	if err != nil {
		return err
	}
	err = ctrl.NewControllerManagedBy(mgr).
		For(&corev1.Pod{}, builder.WithPredicates(
			Not(ManagedByStarboardOperator),
			Not(IsBeingTerminated),
			PodHasContainersReadyCondition,
			installModePredicate,
		)).
		Complete(r.reconcilePods())
	if err != nil {
		return err
	}
	return ctrl.NewControllerManagedBy(mgr).
		For(&batchv1.Job{}, builder.WithPredicates(
			InNamespace(r.Config.Namespace),
			ManagedByStarboardOperator,
			IsVulnerabilityReportScan,
			JobHasAnyCondition,
		)).
		Complete(r.reconcileJobs())
}

func (r *VulnerabilityReportReconciler) reconcilePods() reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("pod", req.NamespacedName)

		pod := &corev1.Pod{}

		// Retrieve the Pod from cache.
		err := r.Get(ctx, req.NamespacedName, pod)
		if err != nil {
			if errors.IsNotFound(err) {
				log.V(1).Info("Ignoring pod that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting pod from cache: %w", err)
		}

		owner := kube.GetImmediateOwnerReference(pod)
		containerImages := kube.GetContainerImagesFromPodSpec(pod.Spec)
		hash := kube.ComputeHash(pod.Spec)

		log.V(1).Info("Resolving workload properties",
			"owner", owner, "hash", hash, "containerImages", containerImages)

		// Check if containers of the Pod have corresponding VulnerabilityReports.
		hasVulnerabilityReports, err := r.hasVulnerabilityReports(ctx, owner, hash, containerImages)
		if err != nil {
			return ctrl.Result{}, fmt.Errorf("getting vulnerability reports: %w", err)
		}

		if hasVulnerabilityReports {
			log.V(1).Info("Ignoring pod that already has VulnerabilityReports", "owner", owner)
			return ctrl.Result{}, nil
		}

		_, job, err := r.hasActiveScanJob(ctx, owner, hash)
		if err != nil {
			return ctrl.Result{}, fmt.Errorf("checking scan job: %w", err)
		}

		if job != nil {
			log.V(1).Info("Scan job already exists",
				"job", fmt.Sprintf("%s/%s", job.Namespace, job.Name),
				"owner", owner)
			return ctrl.Result{}, nil
		}

		limitExceeded, scanJobsCount, err := r.LimitChecker.Check(ctx)
		if err != nil {
			return ctrl.Result{}, err
		}
		log.V(1).Info("Checking scan jobs limit", "count", scanJobsCount, "limit", r.ConcurrentScanJobsLimit)

		if limitExceeded {
			log.V(1).Info("Pushing back scan job", "count", scanJobsCount, "retryAfter", r.ScanJobRetryAfter)
			return ctrl.Result{RequeueAfter: r.Config.ScanJobRetryAfter}, nil
		}

		return ctrl.Result{}, r.submitScanJob(ctx, pod.Spec, owner, containerImages, hash)
	}
}

func (r *VulnerabilityReportReconciler) hasVulnerabilityReports(ctx context.Context, owner kube.Object, hash string, images kube.ContainerImages) (bool, error) {
	list, err := r.FindByOwner(ctx, owner)
	if err != nil {
		return false, err
	}

	actual := map[string]bool{}
	for _, report := range list {
		if containerName, ok := report.Labels[kube.LabelContainerName]; ok {
			if hash == report.Labels[kube.LabelPodSpecHash] {
				actual[containerName] = true
			}
		}
	}

	expected := map[string]bool{}
	for containerName := range images {
		expected[containerName] = true
	}

	return reflect.DeepEqual(actual, expected), nil
}

func (r *VulnerabilityReportReconciler) hasActiveScanJob(ctx context.Context, owner kube.Object, hash string) (bool, *batchv1.Job, error) {
	jobName := fmt.Sprintf("scan-vulnerabilityreport-%s", kube.ComputeHash(owner))
	job := &batchv1.Job{}
	err := r.Get(ctx, client.ObjectKey{Namespace: r.Config.Namespace, Name: jobName}, job)
	if err != nil {
		if errors.IsNotFound(err) {
			return false, nil, nil
		}
		return false, nil, fmt.Errorf("getting job from cache: %w", err)
	}
	if job.Labels[kube.LabelPodSpecHash] == hash {
		return true, job, nil
	}
	return false, nil, nil
}

func (r *VulnerabilityReportReconciler) submitScanJob(ctx context.Context, spec corev1.PodSpec, owner kube.Object, images kube.ContainerImages, hash string) error {
	credentials, err := r.getCredentials(ctx, spec, owner.Namespace)
	if err != nil {
		return err
	}

	templateSpec, secrets, err := r.Plugin.GetScanJobSpec(spec, credentials)
	if err != nil {
		return err
	}

	containerImagesAsJSON, err := images.AsJSON()
	if err != nil {
		return err
	}

	templateSpec.ServiceAccountName = r.Config.ServiceAccount

	for _, secret := range secrets {
		secret.Namespace = r.Config.Namespace
		err := r.Client.Create(ctx, secret)
		if err != nil {
			if errors.IsAlreadyExists(err) {
				return nil
			}
			return fmt.Errorf("creating secret: %w", err)
		}
	}

	jobName := fmt.Sprintf("scan-vulnerabilityreport-%s", kube.ComputeHash(owner))

	scanJob := &batchv1.Job{
		ObjectMeta: metav1.ObjectMeta{
			Name:      jobName,
			Namespace: r.Config.Namespace,
			Labels: map[string]string{
				kube.LabelResourceKind:            string(owner.Kind),
				kube.LabelResourceName:            owner.Name,
				kube.LabelResourceNamespace:       owner.Namespace,
				kube.LabelPodSpecHash:             hash,
				kube.LabelK8SAppManagedBy:         kube.AppStarboardOperator,
				kube.LabelVulnerabilityReportScan: "true",
			},
			Annotations: map[string]string{
				kube.AnnotationContainerImages: containerImagesAsJSON,
			},
		},
		Spec: batchv1.JobSpec{
			BackoffLimit:          pointer.Int32Ptr(0),
			Completions:           pointer.Int32Ptr(1),
			ActiveDeadlineSeconds: kube.GetActiveDeadlineSeconds(r.Config.ScanJobTimeout),
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels: map[string]string{
						kube.LabelResourceKind:            string(owner.Kind),
						kube.LabelResourceName:            owner.Name,
						kube.LabelResourceNamespace:       owner.Namespace,
						kube.LabelPodSpecHash:             hash,
						kube.LabelK8SAppManagedBy:         kube.AppStarboardOperator,
						kube.LabelVulnerabilityReportScan: "true",
					},
				},
				Spec: templateSpec,
			},
		},
	}

	err = r.Client.Create(ctx, scanJob)
	if err != nil {
		return fmt.Errorf("creating job: %w", err)
	}

	for _, secret := range secrets {
		err = controllerutil.SetOwnerReference(scanJob, secret, r.Client.Scheme())
		if err != nil {
			return fmt.Errorf("setting owner reference: %w", err)
		}
		err := r.Client.Update(ctx, secret)
		if err != nil {
			return fmt.Errorf("updating secret: %w", err)
		}
	}

	return nil
}

func (r *VulnerabilityReportReconciler) getCredentials(ctx context.Context, spec corev1.PodSpec, ns string) (map[string]docker.Auth, error) {
	imagePullSecrets, err := r.SecretsReader.ListImagePullSecretsByPodSpec(ctx, spec, ns)
	if err != nil {
		return nil, err
	}
	return kube.MapContainerNamesToDockerAuths(kube.GetContainerImagesFromPodSpec(spec), imagePullSecrets)
}

func (r *VulnerabilityReportReconciler) reconcileJobs() reconcile.Func {
	return func(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
		log := r.Logger.WithValues("job", req.NamespacedName)

		job := &batchv1.Job{}
		err := r.Client.Get(ctx, req.NamespacedName, job)
		if err != nil {
			if errors.IsNotFound(err) {
				log.V(1).Info("Ignoring job that must have been deleted")
				return ctrl.Result{}, nil
			}
			return ctrl.Result{}, fmt.Errorf("getting job from cache: %w", err)
		}

		if len(job.Status.Conditions) == 0 {
			log.V(1).Info("Job has no conditions despite using predicate")
			return ctrl.Result{}, nil
		}

		switch jobCondition := job.Status.Conditions[0].Type; jobCondition {
		case batchv1.JobComplete:
			err = r.processCompleteScanJob(ctx, job)
		case batchv1.JobFailed:
			err = r.processFailedScanJob(ctx, job)
		default:
			err = fmt.Errorf("unrecognized scan job condition: %v", jobCondition)
		}

		return ctrl.Result{}, err
	}

}

func (r *VulnerabilityReportReconciler) processCompleteScanJob(ctx context.Context, job *batchv1.Job) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", job.Namespace, job.Name))
	owner, err := kube.ObjectFromLabelsSet(job.Labels)
	if err != nil {
		return fmt.Errorf("getting workload from scan job labels set: %w", err)
	}

	ownerObj, err := r.GetObjectFromPartialObject(ctx, owner)
	if err != nil {
		return err
	}

	containerImages, err := kube.GetContainerImagesFromJob(job)
	if err != nil {
		return fmt.Errorf("getting container images: %w", err)
	}

	hash, ok := job.Labels[kube.LabelPodSpecHash]
	if !ok {
		return fmt.Errorf("expected label %s not set", kube.LabelPodSpecHash)
	}

	log.V(1).Info("Resolving workload properties",
		"owner", owner, "hash", hash, "containerImages", containerImages)

	hasVulnerabilityReports, err := r.hasVulnerabilityReports(ctx, owner, hash, containerImages)
	if err != nil {
		return err
	}

	if hasVulnerabilityReports {
		log.V(1).Info("VulnerabilityReports already exist", "owner", owner)
		log.V(1).Info("Deleting complete scan job", "owner", owner)
		return r.Client.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground))
	}

	var vulnerabilityReports []v1alpha1.VulnerabilityReport

	for containerName, containerImage := range containerImages {
		logsStream, err := r.LogsReader.GetLogsByJobAndContainerName(ctx, job, containerName)
		if err != nil {
			return fmt.Errorf("getting logs for pod %q: %w", job.Namespace+"/"+job.Name, err)
		}
		scanResult, err := r.Plugin.ParseVulnerabilityScanResult(containerImage, logsStream)
		if err != nil {
			return err
		}
		_ = logsStream.Close()

		report, err := vulnerabilityreport.NewBuilder(r.Client.Scheme()).
			Owner(ownerObj).
			Container(containerName).
			Result(scanResult).
			PodSpecHash(hash).Get()
		if err != nil {
			return err
		}

		vulnerabilityReports = append(vulnerabilityReports, report)
	}

	err = r.ReadWriter.Write(ctx, vulnerabilityReports)
	if err != nil {
		return err
	}

	log.V(1).Info("Deleting complete scan job", "owner", owner)
	return r.Client.Delete(ctx, job, client.PropagationPolicy(metav1.DeletePropagationBackground))
}

func (r *VulnerabilityReportReconciler) processFailedScanJob(ctx context.Context, scanJob *batchv1.Job) error {
	log := r.Logger.WithValues("job", fmt.Sprintf("%s/%s", scanJob.Namespace, scanJob.Name))

	statuses, err := r.GetTerminatedContainersStatusesByJob(ctx, scanJob)
	if err != nil {
		return err
	}
	for container, status := range statuses {
		if status.ExitCode == 0 {
			continue
		}
		log.Error(nil, "Scan job container", "container", container, "status.reason", status.Reason, "status.message", status.Message)
	}
	log.V(1).Info("Deleting failed scan job")
	return r.Client.Delete(ctx, scanJob, client.PropagationPolicy(metav1.DeletePropagationBackground))
}
